{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造：生成一颗完整的决策树。构造的过程就是选择什么属性作为节点的过程。\n",
    "\n",
    "剪枝：给决策树瘦身，防止出现过拟合。其包括预剪枝（Pre-Pruning）和后剪枝（Post-Pruning）。\n",
    "\n",
    "预剪枝：指在决策树构造时进行剪枝。方法是：在决策树构造过程中对节点进行评估，如果该节点在*验证集中*时不能带来准确性的提升，则把该节点当做叶节点（对此为止）。\n",
    "\n",
    "后剪枝：后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。\n",
    "\n",
    "泛化能力：分类器通过训练集抽象出来的分类能力。\n",
    "\n",
    "纯度：目标变量的分歧变小。\n",
    "\n",
    "信息熵（entropy）：信息的不确定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵的数学公式：![](https://static001.geekbang.org/resource/image/74/d5/741f0ed01c53fd53f0e75204542abed5.png)\n",
    "\n",
    "其中：\n",
    "- p(i|t) 代表了节点 t 为分类 i 的概率；\n",
    "\n",
    "- log2 为取以 2 为底的对数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练数据集：![](https://static001.geekbang.org/resource/image/32/07/327eafa4a33e3e76ca86ac59195c0307.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “不纯度”指标 - ID3 算法（信息增益）\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**信息增益**指的就是划分可以带来纯度的提高，信息熵的下降。\n",
    "\n",
    "计算公式就是父亲节点的信息熵减去所有子节点的的信息熵。具体公式如下：![](https://static001.geekbang.org/resource/image/bf/34/bfea7626733fff6180341c9dda3d4334.png)\n",
    "\n",
    "其中：\n",
    "- D 是父亲节点；\n",
    "\n",
    "- Di 是子节点；\n",
    "\n",
    "- Gain(D,a) 中的 a 作为 D 节点的属性选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "案例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://static001.geekbang.org/resource/image/40/67/40810468abc4140f45f3a09a2d427667.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根节点的信息熵是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://static001.geekbang.org/resource/image/9f/9b/9f01e1d1e8082e55850676da50a84f9b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设此时把“天气”当做节点属性来进行计算，则：\n",
    "![](https://static001.geekbang.org/resource/image/85/7f/8537ab10a1a3747d22059bfbbd2aa17f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为“天气”作为子节点，则 D1（晴天）在 D 中出现的概率是 3/7，D2（阴天）在 D 中出现的概率是 2/7，D3（小雨）在 D 中出现的概率是 2/7,。因此，作为子节点的归一化信息熵 = 3/7 * 0.918 + 2/7 * 1.0 + 2/7 * 1.0 = 0.965。故，“天气”作为属性节点的信息增益为 Gain(D,天气)=0.985-0.965=0.02。\n",
    "\n",
    "同理可得：\n",
    "\n",
    "Gain(D,温度) = 0.985-(4/7 * 1.0 + 2/7 * 1.0 + 1/7 * 0)=0.128\n",
    "\n",
    "Gain(D,湿度) = 0.985-(4/7 * 1.0 + 3/7 * 0.918)=0.020\n",
    "\n",
    "Gain(D,刮风) = 0.985-(4/7 * 1.0 + 3/7 * 0.918)=0.020\n",
    "\n",
    "“温度”作为属性的信息增益最大，因此，在此节点层，采用“湿度”作为属性划分。\n",
    "\n",
    "以此类推，可得到完整的决策树如下：\n",
    "（剩下节点的属性计算，其信息熵都是重新开始计算。比如说 D2（温度=高）的信息熵等于1）![](https://s2.ax1x.com/2019/11/16/M02yFO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://static001.geekbang.org/resource/image/11/48/1187ab0048daeec40cd261ea26cd3448.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “不纯度”指标 - C4.5 算法（信息增益率）\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益率 = 信息增益 / 属性熵\n",
    "\n",
    "悲观剪枝：通过递归，估算每个内部节点而分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。\n",
    "\n",
    "离散化处理连续属性：C4.5 选择具有最高信息增益的划分所对应的阈值。\n",
    "\n",
    "可处理缺失值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://static001.geekbang.org/resource/image/50/95/50b43c1820c03561f3ca3e627b454995.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“温度”作为属性时：\n",
    "\n",
    "Gain(D′, 温度)=Ent(D′)-0.792=1.0-0.792=0.208\n",
    "\n",
    "属性熵 =-(3/6 * log2(1/2) + 2/6 * log2(2/6) + 1/6 * log2(1/6)) = 1.459\n",
    "\n",
    "信息增益率 Gain_ratio(D′, 温度)=0.208/1.459=0.1426\n",
    "\n",
    "而因为样本有缺失，D` 的样本个数为 6，而 D 样本的个数为 7。此时：\n",
    "\n",
    "Gain_ratio(D, 温度)=6/7*0.1426=0.122"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://static001.geekbang.org/resource/image/d0/7a/d02e69930c8cf00c93578536933ad07a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 ID3 算法给出好苹果的决策树。\n",
    "\n",
    "![](https://static001.geekbang.org/resource/image/0a/09/0a759fd725add916417c2c294600b609.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s2.ax1x.com/2019/11/16/M0L2LT.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "221.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
