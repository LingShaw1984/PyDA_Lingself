{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T06:03:09.341575Z",
     "start_time": "2019-12-04T06:02:06.033438Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "-*- coding:utf-8 -*-\n",
    "本项目是爬取李宗盛网易云音乐的所有歌曲的歌词，而后实现词云化、关键词提取、词性标注和文本聚类。\n",
    "'''\n",
    "import jieba.analyse\n",
    "import jieba.posseg as pseg\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "import requests\n",
    "import sys\n",
    "import os\n",
    "from lxml import etree\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "伪造请求头，请求网易云歌曲页面的信息\n",
    "'''\n",
    "headers = {\n",
    "    'Referer': 'http://music.163.com',\n",
    "    'Host': 'music.163.com',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'User-Agent': 'Chrome/10'\n",
    "}\n",
    "\n",
    "\n",
    "def get_song_lyric(headers, lyric_url):\n",
    "    res = requests.request('GET', lyric_url, headers=headers)\n",
    "    if 'lrc' in res.json():\n",
    "        lyric = res.json()['lrc']['lyric']\n",
    "        new_lyric = re.sub(r'[\\d:.[\\]]', '', lyric)\n",
    "        return new_lyric\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "'''\n",
    "使用 requests 伪造请求头，获取相关页面数据。\n",
    "再使用 XPath 解析所获数据，然后将解析得到的数据追加到指定列表中。\n",
    "'''\n",
    "\n",
    "\n",
    "def get_songs(artist_id):\n",
    "    page_url = 'https://music.163.com/artist?id=' + artist_id\n",
    "    res = requests.request('GET', page_url, headers=headers)\n",
    "    html = etree.HTML(res.text)\n",
    "    href_xpath = \"//*[@id='hotsong-list']//a/@href\"\n",
    "    name_xpath = \"//*[@id='hotsong-list']//a/text()\"\n",
    "    hrefs = html.xpath(href_xpath)\n",
    "    names = html.xpath(name_xpath)\n",
    "\n",
    "    song_ids = []\n",
    "    song_names = []\n",
    "    for href, name in zip(hrefs, names):\n",
    "        song_ids.append(href[9:])\n",
    "        song_names.append(name)\n",
    "    return song_ids, song_names\n",
    "\n",
    "\n",
    "# 设置歌手对应 ID，以便于请求对应歌手数据\n",
    "artist_id = '3683'\n",
    "[song_ids, song_names] = get_songs(artist_id)\n",
    "\n",
    "all_word = ''\n",
    "for (song_id, song_name) in zip(song_ids, song_names):\n",
    "    # 歌词 API URL\n",
    "    lyric_url = 'http://music.163.com/api/song/lyric?os=pc&id=' + \\\n",
    "        song_id + '&lv=-1&kv=-1&tv=-1'\n",
    "    lyric = get_song_lyric(headers, lyric_url)\n",
    "    all_word = all_word + ' ' + lyric\n",
    "\n",
    "with open('163_maobuying_musci_lyrics.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(all_word)\n",
    "\n",
    "\n",
    "'''\n",
    "停用词使用哈工大停用词表，若有额外停用词，可再额外添加\n",
    "'''\n",
    "\n",
    "\n",
    "def remove_stop_words(f):\n",
    "    with open(\"../../../Data Science/stopwords-master/哈工大停用词表.txt\", encoding=\"utf-8\") as fn:\n",
    "        stop_words = fn.readlines()\n",
    "    stop_words_addition = [\"维度\"]\n",
    "    stop_words = list(stop_words)\n",
    "    stop_words.extend(stop_words_addition)\n",
    "    for stop_word in stop_words:\n",
    "        f = f.replace(stop_word, \"\")\n",
    "    return f\n",
    "\n",
    "\n",
    "'''\n",
    "使用 jieba 库的 analyse 模块提取关键词：\n",
    "提取数量 topK：10；\n",
    "提取关键词的词性：'ns', 'n', 'vn', 'v', 'nr'\n",
    "'''\n",
    "\n",
    "\n",
    "def select_key_words(f):\n",
    "    words = pseg.cut(f.strip())\n",
    "    tags_pairs = jieba.analyse.extract_tags(f,\n",
    "                                            topK=10,\n",
    "                                            withWeight=True,\n",
    "                                            allowPOS=['ns', 'n',\n",
    "                                                      'vn', 'v', 'nr'],\n",
    "                                            withFlag=True)\n",
    "    tags_list = [(i[0].word, i[0].flag, i[1]) for i in tags_pairs]\n",
    "    tags_pd = pd.DataFrame(tags_list, columns=['word', 'flags', 'weight'])\n",
    "    print(\"排名前十的关键词及其词性、权重为：\")\n",
    "    print(tags_pd)\n",
    "\n",
    "\n",
    "'''\n",
    "使用 jieba 库的 posseg 模块标注词性\n",
    "'''\n",
    "\n",
    "\n",
    "def words_POS_tagging(f):\n",
    "    words = pseg.cut(f)\n",
    "    words_pd = pd.DataFrame(words, columns=[\"word\", \"type\"])\n",
    "    words_gb = words_pd.groupby(\n",
    "        ['type'])['word'].count().sort_values(ascending=False)\n",
    "    words_pd_index = words_pd[\"type\"].isin(['ns'])\n",
    "    words_pd_select = words_pd[words_pd_index]\n",
    "    print(\"各类词及其数量如下：\")\n",
    "    print(words_gb)\n",
    "\n",
    "\n",
    "'''\n",
    "使用 jieba 库分词，而后使用 wordcloud 库中的 WordCloud 模块词云化\n",
    "词云化时的参数：\n",
    "设置字体路径，否则无法正常显示中文；\n",
    "设置最大词数为 100；\n",
    "设置词云的尺寸宽×高：2000px × 1200px；\n",
    "设置 collocation 为 True，避免词中出现重复词\n",
    "'''\n",
    "\n",
    "\n",
    "def create_word_cloud(f):\n",
    "    cut_text = \" \".join(jieba.cut(f, cut_all=False, HMM=True))\n",
    "    wc = WordCloud(font_path=\"C:\\Windows\\Fonts\\simhei.ttf\",\n",
    "                   max_words=100,\n",
    "                   width=2000,\n",
    "                   height=1200,\n",
    "                   collocations=False)\n",
    "    word_cloud = wc.generate(cut_text)\n",
    "    picture_path = \"./new_output/wordcloud_wang_yi_yun.jpg\"\n",
    "    word_cloud.to_file(picture_path)\n",
    "    plt.imshow(word_cloud)\n",
    "    plt.axis(\"off\")\n",
    "    print(\"开始生成词云：\")\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "基本思路是：\n",
    "1. 使用 jieba 库中 posseg 模块分割数据\n",
    "2. 只选取某些词性的词\n",
    "3. 计算这些词的 TF-IDF 值\n",
    "4. 将 TF-IDF 转换为空间距离\n",
    "5. 使用 KMeans 算法对这些词进行聚类\n",
    "6. 显示聚类结果\n",
    "'''\n",
    "def word_HanLP(f):\n",
    "    seg_list = pseg.cut(f)\n",
    "    word_list = [i.word for i in seg_list if i.flag in ['a', 'ag', 'an']]\n",
    "    vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    X = vectorizer.fit_transform(word_list)\n",
    "\n",
    "    model_kmeans = KMeans(n_clusters=3)\n",
    "    model_kmeans.fit(X)\n",
    "\n",
    "    cluster_labels = model_kmeans.labels_\n",
    "    word_vectors = vectorizer.get_feature_names()\n",
    "    word_value = X.toarray()\n",
    "\n",
    "    f_matrix = np.hstack(word_value,\n",
    "                         cluster_labels.reshape(word_value.shape[0], 1))\n",
    "    word_vectors.append('cluster_labels')\n",
    "    comment_pd = pd.DataFrame(comment_matrix,\n",
    "                              columns=word_vectors)\n",
    "    word_importance = np.sum(comment_cluster1, axis=0)\n",
    "    print(word_importance)\n",
    "\n",
    "\n",
    "all_word = remove_stop_words(all_word)\n",
    "# select_key_words(all_word)\n",
    "# words_POS_tagging(all_word)\n",
    "create_word_cloud(all_word)\n",
    "word_HanLP(all_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目存在一定缺陷，如下：\n",
    "数据获取方面：由于只在网易云音乐获取数据，因此可能出现歌曲信息获取不完整的情况。如果要解决这个问题，其实第一步要做的事，是把李宗盛所有歌曲做个调查，然后再对比所爬取的歌曲信息，看是否存在数据缺漏问题。<br>还有个折中法子，就是把李宗盛唱片销售量排前几的歌曲全部下载下来，然后再分析，即取代表性歌曲做分析。这里采取的思路也是这样，不过是爬取网易云李宗盛前 50 首热门歌曲信息。\n",
    "\n",
    "数据清洗：实际上，前 50 首歌曲中，很有可能出现重复歌曲，只是版本不一样。因此，这就出现数据重复采集。如有可能，首先应该观察要爬取的歌曲数据，然后再选择是否跳过某些歌曲。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
